{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnsonYu0924/114_2_text-analysis/blob/main/main_cleaner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "ee54ea0b",
      "metadata": {
        "id": "ee54ea0b",
        "outputId": "99cedb57-f461-404e-eb90-84714ba5a997",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import json, re, os, csv\n",
        "\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U ckip-transformers\n",
        "\n",
        "# import tools: token(WS), tagging(POS), 實體辨識(NER)\n",
        "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
        "\n",
        "# diver initiation(初始化一次之後先關一下)\n",
        "WS_driver = CkipWordSegmenter(model='bert-base')\n",
        "POS_driver = CkipPosTagger(model='bert-base')\n",
        "NER_driver = CkipNerChunker(model='bert-base')"
      ],
      "metadata": {
        "id": "AfXymCAAGNhn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adbad787-116b-46ae-a08e-b54f0a1bcc12"
      },
      "id": "AfXymCAAGNhn",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import stopwords\n",
        "with open(\"/content/stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  stopwords = f.read().split(\"\\n\")\n",
        "  stopwords = set(stopwords)"
      ],
      "metadata": {
        "id": "DcQGJDW-2M8n"
      },
      "id": "DcQGJDW-2M8n",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenCleaner(file_name, import_path, outport_path):\n",
        "  \"\"\"\n",
        "  import .json file\n",
        "  dealing with: <https://>, punctuations, (numbers, #option), lower\n",
        "\n",
        "  利用CKIP進行tokenize,並利用stopwords來清理。\n",
        "  file_name放入含有.json的檔名就可以開始使用...\n",
        "\n",
        "  使用前先確認\n",
        "    (1) ckip WS driver是否已經初始化\n",
        "    (2) topwords辭典是否有放進工作區\n",
        "    (3) 路徑是否有正確：os.chdir(path)\n",
        "\n",
        "  export .json file\n",
        "  \"\"\"\n",
        "  with open(import_path + file_name, 'r', encoding='utf-8') as f:\n",
        "    transcript = json.load(f)\n",
        "    print(f\"{file_name} opened success!\")\n",
        "\n",
        "  cleanedTrans = []\n",
        "  for i in transcript:\n",
        "    i = re.sub(r\"https://\\S+\", \" \", i)\n",
        "    i = re.sub(r\"[!\\[\\]:「」『』《》〈〉（）\\(\\)，、：；。\\.]\", \" \", i)\n",
        "    # i = re.sub(r\"\\d+\", \"\", i) #可以考慮要不要處理數字，跟情緒沒什麼關係\n",
        "    i = i.strip().lower()\n",
        "    cleanedTrans.append(i)\n",
        "  print(f\"{file_name} cleaned!\\n{cleanedTrans[:5]}\")\n",
        "\n",
        "  tokenedTrans = WS_driver(cleanedTrans)\n",
        "  print(f\"{file_name} tokened by WS!\\n{tokenedTrans[:10]}\")\n",
        "\n",
        "  # Trans = []\n",
        "  # for i in tokenedTrans:\n",
        "  #   for j in i:\n",
        "  #     if j not in stopwords:\n",
        "  #       tokenedTrans.append(j)\n",
        "  # print(f\"{file_name} cleaned!\\n{Trans[:10]}\")\n",
        "\n",
        "  file_name_w = re.sub(r\"\\.json\", \"\", file_name)\n",
        "  file_name_w = file_name_w + '_tokened.json'\n",
        "  with open(outport_path + file_name_w, 'w', encoding='utf-8') as f:\n",
        "    json.dump(tokenedTrans, f, ensure_ascii=False, indent=4)\n",
        "    os.close"
      ],
      "metadata": {
        "id": "ecHWGnxyaduh"
      },
      "id": "ecHWGnxyaduh",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import_path = \"/content/raw/\"\n",
        "outport_path = \"/content/tokened/\"\n",
        "\n",
        "dirlist = [i for i in os.listdir(import_path) if i.endswith(\".json\")]\n",
        "for jdata in dirlist:\n",
        "  tokenCleaner(jdata, import_path, outport_path)\n",
        "  print(f\"{jdata} done!\\n-------------\")"
      ],
      "metadata": {
        "id": "PxitWelbIwkG",
        "outputId": "23133e64-b470-481b-dae8-c7abdf2cedcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "id": "PxitWelbIwkG",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'os' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-768325587.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moutport_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/tokened/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdirlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimport_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdirlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mtokenCleaner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimport_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutport_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
