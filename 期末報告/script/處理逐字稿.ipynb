{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnsonYu0924/114_2_text-analysis/blob/main/%E6%9C%9F%E6%9C%AB%E5%A0%B1%E5%91%8A/script/%E8%99%95%E7%90%86%E9%80%90%E5%AD%97%E7%A8%BF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 0：環境設定"
      ],
      "metadata": {
        "id": "hykoSUUhafeC"
      },
      "id": "hykoSUUhafeC"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "ee54ea0b",
      "metadata": {
        "id": "ee54ea0b",
        "outputId": "38d48f48-99c0-4327-9743-5d2a6f0ce893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import json, re, os, csv\n",
        "import re\n",
        "\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1：抓取逐字稿"
      ],
      "metadata": {
        "id": "nRLATQu_ak1q"
      },
      "id": "nRLATQu_ak1q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d0ffba9",
      "metadata": {
        "id": "4d0ffba9"
      },
      "outputs": [],
      "source": [
        "# 先寫看看\n",
        "\n",
        "with open('191218_政見發表會.md', 'r', encoding='utf-8') as t:\n",
        "    transcript = t.read()\n",
        "    print(type(transcript)) #class\n",
        "\n",
        "senten = []\n",
        "transcript = re.split(r\"\\n\", transcript)\n",
        "for i in transcript:\n",
        "    if i != \"\":\n",
        "        senten.append(i.strip())\n",
        "\n",
        "candidate = input(\"要查詢的人：\")\n",
        "target_candidate = '#### ' + candidate\n",
        "candidateTranscript = []\n",
        "is_collecting = False\n",
        "\n",
        "for front, later in zip(senten[:-1], senten[1:]):\n",
        "  if front == target_candidate:\n",
        "    is_collecting = not is_collecting\n",
        "  if is_collecting and not re.search(r\"###\", later):\n",
        "    candidateTranscript.append(later)\n",
        "    # print(later)\n",
        "  elif is_collecting and re.search(r\"###\", later):\n",
        "    is_collecting = not is_collecting\n",
        "\n",
        "for i in candidateTranscript:\n",
        "  print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 請Gemini幫我變成def好處理\n",
        "\n",
        "import re\n",
        "import os\n",
        "\n",
        "def candiTrans(filename, candidate):\n",
        "    \"\"\"\n",
        "    從指定的 Markdown 檔案中提取特定候選人的發言內容。\n",
        "\n",
        "    發言區段的判斷邏輯是：\n",
        "    1. 遇到 '#### [候選人姓名]' 標記，開啟/關閉收集狀態 (is_collecting)。\n",
        "    2. 在收集狀態下，直到下一個包含 '###' 的行 (代表另一位候選人發言的開始)\n",
        "       出現為止，收集該區間的所有內容。\n",
        "\n",
        "    Args:\n",
        "        filename (str): 要讀取的檔案名稱。\n",
        "        candidate (str): 要查詢的候選人姓名 (例如: '韓國瑜' 或 '蔡英文')。\n",
        "\n",
        "    Returns:\n",
        "        list: 包含該候選人所有發言句子的列表。\n",
        "    \"\"\"\n",
        "\n",
        "    # 檢查檔案是否存在\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"錯誤：檔案 '{filename}' 不存在。\")\n",
        "        return []\n",
        "\n",
        "    # 1. 檔案讀取與文本清洗\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as t:\n",
        "            transcript = t.read()\n",
        "    except Exception as e:\n",
        "        print(f\"讀取檔案時發生錯誤: {e}\")\n",
        "        return []\n",
        "\n",
        "    # 將文本按換行符分割並清除空白行和行首尾空白\n",
        "    transcript_lines = re.split(r\"\\n\", transcript)\n",
        "    senten = []\n",
        "    for line in transcript_lines:\n",
        "        stripped_line = line.strip()\n",
        "        if stripped_line != \"\":\n",
        "            senten.append(stripped_line)\n",
        "\n",
        "    # 2. 設定目標分隔符號\n",
        "    target_candidate = '#### ' + candidate\n",
        "    candidateTranscript = []\n",
        "    is_collecting = False\n",
        "\n",
        "    # 3. 遍歷並提取發言內容 (使用 zip 進行前/後配對)\n",
        "    # 注意: 這個邏輯不包含區段開頭和結尾的分隔符號本身。\n",
        "    # 並且，這裡假設所有分隔符號都至少包含 '###'。\n",
        "    for front, later in zip(senten[:-1], senten[1:]):\n",
        "\n",
        "        # 檢查是否遇到目標候選人發言的開頭 (例如: '#### 韓國瑜')\n",
        "        if front == target_candidate:\n",
        "            # 切換狀態：False -> True (開始收集) 或 True -> False (結束收集)\n",
        "            is_collecting = not is_collecting\n",
        "\n",
        "        # 如果正在收集且下一句不是分隔符號\n",
        "        if is_collecting and not re.search(r\"###\", later):\n",
        "            candidateTranscript.append(later)\n",
        "\n",
        "        # 如果正在收集且下一句是分隔符號 (結束收集)\n",
        "        elif is_collecting and re.search(r\"###\", later):\n",
        "            # 切換狀態：True -> False (停止收集)\n",
        "            is_collecting = not is_collecting\n",
        "\n",
        "    return candidateTranscript"
      ],
      "metadata": {
        "id": "1e8mA51kTdUm"
      },
      "id": "1e8mA51kTdUm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 調用function\n",
        "\n",
        "file_name = input(\"請輸入檔案名稱：\")\n",
        "file_name = file_name + '.md'\n",
        "\n",
        "candidate_name = input(\"請輸入要查詢的人：\")\n",
        "\n",
        "result_list = candiTrans(file_name, candidate_name)\n",
        "\n",
        "if result_list:\n",
        "    for item in result_list:\n",
        "        print(item)\n",
        "else:\n",
        "    print(\"未找到發言內容。\")\n"
      ],
      "metadata": {
        "id": "IRTu8mkcusR0"
      },
      "id": "IRTu8mkcusR0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3：導出為.json檔"
      ],
      "metadata": {
        "id": "IE7qC8rDeEGH"
      },
      "id": "IE7qC8rDeEGH"
    },
    {
      "cell_type": "code",
      "source": [
        "jsonDataName = input('輸入檔名（eg. 191218_Tsai/Han/Soong): ')\n",
        "jsonDataName = jsonDataName + \".json\"\n",
        "\n",
        "with open(jsonDataName, 'w', encoding='utf-8') as d:\n",
        "  json.dump(result_list, d, ensure_ascii=False, indent=4)\n",
        "\n",
        "#重複做把所有.md檔整理成.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtTymFBCej7S",
        "outputId": "10df2c20-5f4f-4d51-9544-f1a95af23ca3"
      },
      "id": "DtTymFBCej7S",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "輸入檔名（eg. 191218_Tsai/Han/Soong): 191229_Soong\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 4：清理逐字稿"
      ],
      "metadata": {
        "id": "i8i6gPHza_Kj"
      },
      "id": "i8i6gPHza_Kj"
    },
    {
      "cell_type": "code",
      "source": [
        "def basicCleaner(file_name):\n",
        "  # file_name = '191227_Han.json'\n",
        "  file_name_w = re.sub(r\"\\.json\", \"\", file_name)\n",
        "\n",
        "  with open(file_name, 'r', encoding='utf-8') as f:\n",
        "    transcript = json.load(f)\n",
        "    print(transcript)\n",
        "\n",
        "  cleanTrans = []\n",
        "  for i in transcript:\n",
        "    i = re.sub(r\"https://\\S+\", \" \", i)\n",
        "    i = re.sub(r\"[!\\[\\]:「」『』《》〈〉（）\\(\\)，、：；。\\.]\", \" \", i)\n",
        "    # i = re.sub(r\"\\d+\", \"\", i) #可以考慮要不要處理數字，跟情緒沒什麼關係\n",
        "    i = i.strip().lower()\n",
        "    cleanTrans.append(i)\n",
        "\n",
        "  for i in cleanTrans:\n",
        "    print(i)\n",
        "\n",
        "  file_name_w = file_name_w + '_cleaned.json'\n",
        "  with open(file_name_w, 'w', encoding='utf-8') as f:\n",
        "    json.dump(cleanTrans, f, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "id": "ecHWGnxyaduh"
      },
      "id": "ecHWGnxyaduh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 調用function\n",
        "\n",
        "# fileName = input(\"請輸入檔案名稱（含.json）：\")\n",
        "# basicCleaner(fileName)\n",
        "\n",
        "# 批次處理\n",
        "path = \"/content/\"\n",
        "print(os.getcwd())\n",
        "dirList = [i for i in os.listdir(path) if i.endswith(\".json\")]\n",
        "if len(dirList) == 0:\n",
        "  print(\"There's no '.json' data.\")\n",
        "else:\n",
        "  print(dirList)\n",
        "\n",
        "for i in dirList:\n",
        "  basicCleaner(i)"
      ],
      "metadata": {
        "id": "6s-DytCnY2Cx"
      },
      "id": "6s-DytCnY2Cx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#刪除資料\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# ⚠️ 請將這裡替換成您要清理的資料夾路徑\n",
        "folder_path = '/content'\n",
        "\n",
        "# 定義要刪除的檔名模式\n",
        "patterns_to_delete = ['*_cleaned.json', '*_cleaned_cleaned.json']\n",
        "\n",
        "# 遍歷所有模式並刪除檔案\n",
        "deleted_count = 0\n",
        "for pattern in patterns_to_delete:\n",
        "    # 組合完整的搜索路徑\n",
        "    search_path = os.path.join(folder_path, pattern)\n",
        "\n",
        "    # glob.glob() 找到所有符合模式的檔案路徑\n",
        "    for file_path in glob.glob(search_path):\n",
        "        try:\n",
        "            # 刪除檔案\n",
        "            os.remove(file_path)\n",
        "            print(f\"✅ 成功刪除: {file_path}\")\n",
        "            deleted_count += 1\n",
        "        except OSError as e:\n",
        "            # 處理刪除失敗的情況（例如權限不足）\n",
        "            print(f\"❌ 錯誤：無法刪除 {file_path}. {e}\")\n",
        "\n",
        "print(f\"\\n--- 總結：共刪除了 {deleted_count} 個檔案。 ---\")"
      ],
      "metadata": {
        "id": "DbgJHu4fcAsU",
        "outputId": "a489e560-9567-4549-b62b-ff6404af4acd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DbgJHu4fcAsU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 成功刪除: /content/191227_Soong_cleaned.json\n",
            "✅ 成功刪除: /content/191218_Tsai_cleaned.json\n",
            "✅ 成功刪除: /content/191227_Han_cleaned.json\n",
            "✅ 成功刪除: /content/191229_Han_cleaned.json\n",
            "✅ 成功刪除: /content/191229_Soong_cleaned.json\n",
            "✅ 成功刪除: /content/191225_Han_cleaned.json\n",
            "✅ 成功刪除: /content/191227_Tsai_cleaned.json\n",
            "✅ 成功刪除: /content/191218_Han_cleaned.json\n",
            "✅ 成功刪除: /content/191229_Tsai_cleaned.json\n",
            "✅ 成功刪除: /content/191225_Soong_cleaned.json\n",
            "✅ 成功刪除: /content/191218_Soong_cleaned.json\n",
            "✅ 成功刪除: /content/191225_Tsai_cleaned.json\n",
            "\n",
            "--- 總結：共刪除了 12 個檔案。 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 5：切token\n",
        "利用中研院「CKIP Transformers」代替「jieba」進行token處理"
      ],
      "metadata": {
        "id": "vQjZJH5iQPS6"
      },
      "id": "vQjZJH5iQPS6"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U ckip-transformers\n",
        "\n",
        "# import tools: token(WS), tagging(POS), 實體辨識(NER)\n",
        "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
        "\n",
        "# diver initiation(初始化一次之後先關一下)\n",
        "WS_driver = CkipWordSegmenter(model='bert-base')\n",
        "POS_driver = CkipPosTagger(model='bert-base')\n",
        "NER_driver = CkipNerChunker(model='bert-base')\n",
        "\n",
        "# imput data\n",
        "import os, json\n",
        "with open(\"/content/191218_Han_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  f = json.load(f)\n",
        "  myText = \"\".join(f)\n",
        "print(myText[:5])\n",
        "\n",
        "# run\n",
        "myText_WS = WS_driver(myText)\n",
        "myText_POS = POS_driver(myText_WS) # Changed to pass myText_WS to POS_driver\n",
        "myText_NER = NER_driver(myText)\n",
        "\n",
        "# print result\n",
        "print(\"\\nWS \\n\", myText_WS)\n",
        "print(\"POS \\n\", myText_POS)\n",
        "print(\"NER \\n\", myText_NER)\n",
        "\n",
        "\n",
        "# 引入stopwords dictionary\n",
        "with open(\"/content/stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  stopwords = f.read().split(\"\\n\")\n",
        "  stopwords = set(stopwords)\n",
        "\n",
        "# filtered = []\n",
        "# for i in range(len(myText_WS)):\n",
        "\n"
      ],
      "metadata": {
        "id": "PxitWelbIwkG",
        "outputId": "af5527a7-d258-4161-937c-0fdfbaa1bdcb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "id": "PxitWelbIwkG",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ckip-transformers in /usr/local/lib/python3.12/dist-packages (0.3.4)\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from ckip-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from ckip-transformers) (4.67.1)\n",
            "Requirement already satisfied: transformers>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from ckip-transformers) (4.57.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.5.0->ckip-transformers) (3.5.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.5.0->ckip-transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.5.0->ckip-transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.5.0->ckip-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.5.0->ckip-transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.5.0->ckip-transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers>=3.5.0->ckip-transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.5.0->ckip-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.5.0->ckip-transformers) (0.7.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.5.0->ckip-transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.5.0->ckip-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.5.0->ckip-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=3.5.0->ckip-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=3.5.0->ckip-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=3.5.0->ckip-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers>=3.5.0->ckip-transformers) (2025.11.12)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3743849245.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mWS_driver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCkipWordSegmenter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bert-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mPOS_driver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCkipPosTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bert-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mNER_driver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCkipNerChunker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bert-base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# imput data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ckip_transformers/nlp/driver.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, **kwargs)\u001b[0m\n\u001b[1;32m    241\u001b[0m     ):\n\u001b[1;32m    242\u001b[0m         \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     def __call__(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ckip_transformers/nlp/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_name, tokenizer_name, device)\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     ):\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForTokenClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_name\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    603\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    605\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5046\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5047\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5048\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5049\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5050\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5314\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5315\u001b[0m             original_checkpoint_keys = list(\n\u001b[0;32m-> 5316\u001b[0;31m                 \u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_files\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"meta\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5317\u001b[0m             )\n\u001b[1;32m   5318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmap_location\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"meta\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"mmap\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m         return torch.load(\n\u001b[0m\u001b[1;32m    527\u001b[0m             \u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m                         return _load(\n\u001b[0m\u001b[1;32m   1522\u001b[0m                             \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                             \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   2120\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2121\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2122\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2123\u001b[0m     \u001b[0m_serialization_tls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_location\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_weights_only_unpickler.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    533\u001b[0m                         \u001b[0;34mf\"Only persistent_load of storage is allowed, but got {pid[0]}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m                     )\n\u001b[0;32m--> 535\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    536\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mBINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLONG_BINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mBINGET\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0munpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"<I\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mpersistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   2084\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2085\u001b[0m             \u001b[0mnbytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumel\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2086\u001b[0;31m             typed_storage = load_tensor(\n\u001b[0m\u001b[1;32m   2087\u001b[0m                 \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_maybe_decode_ascii\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2088\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   2037\u001b[0m                     )\n\u001b[1;32m   2038\u001b[0m             storage = (\n\u001b[0;32m-> 2039\u001b[0;31m                 \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_storage_from_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUntypedStorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2040\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0m_typed_storage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2041\u001b[0m                 \u001b[0;34m.\u001b[0m\u001b[0m_untyped_storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 暫存區"
      ],
      "metadata": {
        "id": "VgxOYM-xc-nA"
      },
      "id": "VgxOYM-xc-nA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question: 能不能把上面操作變成批次處理？（好瘋）"
      ],
      "metadata": {
        "id": "pz4h43Ajetpc"
      },
      "id": "pz4h43Ajetpc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b09fccbc",
      "metadata": {
        "id": "b09fccbc",
        "outputId": "edb739fd-9c4c-4865-8a7a-77b9b661343c",
        "collapsed": true
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '1218.html'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[59], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 從HTHL清資料\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m HTML1218 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1218.html\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(HTML1218, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print(soup)\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1218.html'"
          ]
        }
      ],
      "source": [
        "# 從HTHL清資料（先不用這個東西）\n",
        "\n",
        "HTML1218 = open('1218.html', 'r', encoding='utf-8')\n",
        "soup = BeautifulSoup(HTML1218, 'html.parser')\n",
        "# print(soup)\n",
        "\n",
        "\n",
        "all_spans = soup.find_all('span')\n",
        "content_list = []\n",
        "flag = False\n",
        "\n",
        "for span in all_spans:\n",
        "    text = span.get_text(strip=True)\n",
        "    if text == '逐字稿：':\n",
        "        flag = True\n",
        "    if flag and text:\n",
        "        clean_text = re.sub(r'[\\u3000\\s]+', ' ', text).strip()\n",
        "        print(text)\n",
        "    # if clean_text and clean_text != ' ':\n",
        "    #     content_list.append(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 列印token和tag的結果\n",
        "\n",
        "def pack_ws_pos_sentece(sentence_ws, sentence_pos):\n",
        "    assert len(sentence_ws) == len(sentence_pos)\n",
        "    res = []\n",
        "    for word_ws, word_pos in zip(sentence_ws, sentence_pos):\n",
        "        res.append(f\"{word_ws}({word_pos})\")\n",
        "    return \"\\u3000\".join(res)\n",
        "\n",
        "for myText, ws, pos in zip(myText, myText_WS, myText_POS):\n",
        "  print(myText)\n",
        "  print(pack_ws_pos_sentece(ws, pos))"
      ],
      "metadata": {
        "id": "xQUoP8843-KF",
        "outputId": "7d961b44-a2a5-47b5-8447-777eefa8725b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "id": "xQUoP8843-KF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'myText' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3027488440.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\\u3000\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmyText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyText_WS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyText_POS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_ws_pos_sentece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'myText' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TQ96_W-t3_W2"
      },
      "id": "TQ96_W-t3_W2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}