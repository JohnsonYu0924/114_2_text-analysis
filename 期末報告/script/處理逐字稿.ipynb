{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnsonYu0924/114_2_text-analysis/blob/main/%E6%9C%9F%E6%9C%AB%E5%A0%B1%E5%91%8A/script/%E8%99%95%E7%90%86%E9%80%90%E5%AD%97%E7%A8%BF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 0：環境設定"
      ],
      "metadata": {
        "id": "hykoSUUhafeC"
      },
      "id": "hykoSUUhafeC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee54ea0b",
      "metadata": {
        "id": "ee54ea0b",
        "outputId": "8cbedfbe-5ac0-438d-8692-536c7486270d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import json, re, os, csv\n",
        "import re\n",
        "\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1：抓取逐字稿"
      ],
      "metadata": {
        "id": "nRLATQu_ak1q"
      },
      "id": "nRLATQu_ak1q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d0ffba9",
      "metadata": {
        "id": "4d0ffba9"
      },
      "outputs": [],
      "source": [
        "# 先寫看看\n",
        "\n",
        "with open('191218_政見發表會.md', 'r', encoding='utf-8') as t:\n",
        "    transcript = t.read()\n",
        "    print(type(transcript)) #class\n",
        "\n",
        "senten = []\n",
        "transcript = re.split(r\"\\n\", transcript)\n",
        "for i in transcript:\n",
        "    if i != \"\":\n",
        "        senten.append(i.strip())\n",
        "\n",
        "candidate = input(\"要查詢的人：\")\n",
        "target_candidate = '#### ' + candidate\n",
        "candidateTranscript = []\n",
        "is_collecting = False\n",
        "\n",
        "for front, later in zip(senten[:-1], senten[1:]):\n",
        "  if front == target_candidate:\n",
        "    is_collecting = not is_collecting\n",
        "  if is_collecting and not re.search(r\"###\", later):\n",
        "    candidateTranscript.append(later)\n",
        "    # print(later)\n",
        "  elif is_collecting and re.search(r\"###\", later):\n",
        "    is_collecting = not is_collecting\n",
        "\n",
        "for i in candidateTranscript:\n",
        "  print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 請Gemini幫我變成def好處理\n",
        "\n",
        "import re\n",
        "import os\n",
        "\n",
        "def candiTrans(filename, candidate):\n",
        "    \"\"\"\n",
        "    從指定的 Markdown 檔案中提取特定候選人的發言內容。\n",
        "\n",
        "    發言區段的判斷邏輯是：\n",
        "    1. 遇到 '#### [候選人姓名]' 標記，開啟/關閉收集狀態 (is_collecting)。\n",
        "    2. 在收集狀態下，直到下一個包含 '###' 的行 (代表另一位候選人發言的開始)\n",
        "       出現為止，收集該區間的所有內容。\n",
        "\n",
        "    Args:\n",
        "        filename (str): 要讀取的檔案名稱。\n",
        "        candidate (str): 要查詢的候選人姓名 (例如: '韓國瑜' 或 '蔡英文')。\n",
        "\n",
        "    Returns:\n",
        "        list: 包含該候選人所有發言句子的列表。\n",
        "    \"\"\"\n",
        "\n",
        "    # 檢查檔案是否存在\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"錯誤：檔案 '{filename}' 不存在。\")\n",
        "        return []\n",
        "\n",
        "    # 1. 檔案讀取與文本清洗\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as t:\n",
        "            transcript = t.read()\n",
        "    except Exception as e:\n",
        "        print(f\"讀取檔案時發生錯誤: {e}\")\n",
        "        return []\n",
        "\n",
        "    # 將文本按換行符分割並清除空白行和行首尾空白\n",
        "    transcript_lines = re.split(r\"\\n\", transcript)\n",
        "    senten = []\n",
        "    for line in transcript_lines:\n",
        "        stripped_line = line.strip()\n",
        "        if stripped_line != \"\":\n",
        "            senten.append(stripped_line)\n",
        "\n",
        "    # 2. 設定目標分隔符號\n",
        "    target_candidate = '#### ' + candidate\n",
        "    candidateTranscript = []\n",
        "    is_collecting = False\n",
        "\n",
        "    # 3. 遍歷並提取發言內容 (使用 zip 進行前/後配對)\n",
        "    # 注意: 這個邏輯不包含區段開頭和結尾的分隔符號本身。\n",
        "    # 並且，這裡假設所有分隔符號都至少包含 '###'。\n",
        "    for front, later in zip(senten[:-1], senten[1:]):\n",
        "\n",
        "        # 檢查是否遇到目標候選人發言的開頭 (例如: '#### 韓國瑜')\n",
        "        if front == target_candidate:\n",
        "            # 切換狀態：False -> True (開始收集) 或 True -> False (結束收集)\n",
        "            is_collecting = not is_collecting\n",
        "\n",
        "        # 如果正在收集且下一句不是分隔符號\n",
        "        if is_collecting and not re.search(r\"###\", later):\n",
        "            candidateTranscript.append(later)\n",
        "\n",
        "        # 如果正在收集且下一句是分隔符號 (結束收集)\n",
        "        elif is_collecting and re.search(r\"###\", later):\n",
        "            # 切換狀態：True -> False (停止收集)\n",
        "            is_collecting = not is_collecting\n",
        "\n",
        "    return candidateTranscript"
      ],
      "metadata": {
        "id": "1e8mA51kTdUm"
      },
      "id": "1e8mA51kTdUm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 調用function\n",
        "\n",
        "file_name = input(\"請輸入檔案名稱：\")\n",
        "file_name = file_name + '.md'\n",
        "\n",
        "candidate_name = input(\"請輸入要查詢的人：\")\n",
        "\n",
        "result_list = candiTrans(file_name, candidate_name)\n",
        "\n",
        "if result_list:\n",
        "    for item in result_list:\n",
        "        print(item)\n",
        "else:\n",
        "    print(\"未找到發言內容。\")\n"
      ],
      "metadata": {
        "id": "IRTu8mkcusR0"
      },
      "id": "IRTu8mkcusR0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3：導出為.json檔"
      ],
      "metadata": {
        "id": "IE7qC8rDeEGH"
      },
      "id": "IE7qC8rDeEGH"
    },
    {
      "cell_type": "code",
      "source": [
        "jsonDataName = input('輸入檔名（eg. 191218_Tsai/Han/Soong): ')\n",
        "jsonDataName = jsonDataName + \".json\"\n",
        "\n",
        "with open(jsonDataName, 'w', encoding='utf-8') as d:\n",
        "  json.dump(result_list, d, ensure_ascii=False, indent=4)\n",
        "\n",
        "#重複做把所有.md檔整理成.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtTymFBCej7S",
        "outputId": "10df2c20-5f4f-4d51-9544-f1a95af23ca3"
      },
      "id": "DtTymFBCej7S",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "輸入檔名（eg. 191218_Tsai/Han/Soong): 191229_Soong\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 4：清理逐字稿"
      ],
      "metadata": {
        "id": "i8i6gPHza_Kj"
      },
      "id": "i8i6gPHza_Kj"
    },
    {
      "cell_type": "code",
      "source": [
        "def basicCleaner(file_name):\n",
        "  # file_name = '191227_Han.json'\n",
        "  file_name_w = re.sub(r\"\\.json\", \"\", file_name)\n",
        "\n",
        "  with open(file_name, 'r', encoding='utf-8') as f:\n",
        "    transcript = json.load(f)\n",
        "    print(transcript)\n",
        "\n",
        "  cleanTrans = []\n",
        "  for i in transcript:\n",
        "    i = re.sub(r\"https://\\S+\", \" \", i)\n",
        "    i = re.sub(r\"[!\\[\\]:「」『』《》〈〉（）\\(\\)，、：；。\\.]\", \" \", i)\n",
        "    # i = re.sub(r\"\\d+\", \"\", i) #可以考慮要不要處理數字，跟情緒沒什麼關係\n",
        "    i = i.strip().lower()\n",
        "    cleanTrans.append(i)\n",
        "\n",
        "  for i in cleanTrans:\n",
        "    print(i)\n",
        "\n",
        "  file_name_w = file_name_w + '_cleaned.json'\n",
        "  with open(file_name_w, 'w', encoding='utf-8') as f:\n",
        "    json.dump(cleanTrans, f, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "id": "ecHWGnxyaduh"
      },
      "id": "ecHWGnxyaduh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 調用function\n",
        "\n",
        "# fileName = input(\"請輸入檔案名稱（含.json）：\")\n",
        "# basicCleaner(fileName)\n",
        "\n",
        "# 批次處理\n",
        "path = \"/content/\"\n",
        "print(os.getcwd())\n",
        "dirList = [i for i in os.listdir(path) if i.endswith(\".json\")]\n",
        "if len(dirList) == 0:\n",
        "  print(\"There's no '.json' data.\")\n",
        "else:\n",
        "  print(dirList)\n",
        "\n",
        "for i in dirList:\n",
        "  basicCleaner(i)"
      ],
      "metadata": {
        "id": "6s-DytCnY2Cx"
      },
      "id": "6s-DytCnY2Cx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#刪除資料\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# ⚠️ 請將這裡替換成您要清理的資料夾路徑\n",
        "folder_path = '/content'\n",
        "\n",
        "# 定義要刪除的檔名模式\n",
        "patterns_to_delete = ['*_cleaned.json', '*_cleaned_cleaned.json']\n",
        "\n",
        "# 遍歷所有模式並刪除檔案\n",
        "deleted_count = 0\n",
        "for pattern in patterns_to_delete:\n",
        "    # 組合完整的搜索路徑\n",
        "    search_path = os.path.join(folder_path, pattern)\n",
        "\n",
        "    # glob.glob() 找到所有符合模式的檔案路徑\n",
        "    for file_path in glob.glob(search_path):\n",
        "        try:\n",
        "            # 刪除檔案\n",
        "            os.remove(file_path)\n",
        "            print(f\"✅ 成功刪除: {file_path}\")\n",
        "            deleted_count += 1\n",
        "        except OSError as e:\n",
        "            # 處理刪除失敗的情況（例如權限不足）\n",
        "            print(f\"❌ 錯誤：無法刪除 {file_path}. {e}\")\n",
        "\n",
        "print(f\"\\n--- 總結：共刪除了 {deleted_count} 個檔案。 ---\")"
      ],
      "metadata": {
        "id": "DbgJHu4fcAsU",
        "outputId": "a489e560-9567-4549-b62b-ff6404af4acd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DbgJHu4fcAsU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 成功刪除: /content/191227_Soong_cleaned.json\n",
            "✅ 成功刪除: /content/191218_Tsai_cleaned.json\n",
            "✅ 成功刪除: /content/191227_Han_cleaned.json\n",
            "✅ 成功刪除: /content/191229_Han_cleaned.json\n",
            "✅ 成功刪除: /content/191229_Soong_cleaned.json\n",
            "✅ 成功刪除: /content/191225_Han_cleaned.json\n",
            "✅ 成功刪除: /content/191227_Tsai_cleaned.json\n",
            "✅ 成功刪除: /content/191218_Han_cleaned.json\n",
            "✅ 成功刪除: /content/191229_Tsai_cleaned.json\n",
            "✅ 成功刪除: /content/191225_Soong_cleaned.json\n",
            "✅ 成功刪除: /content/191218_Soong_cleaned.json\n",
            "✅ 成功刪除: /content/191225_Tsai_cleaned.json\n",
            "\n",
            "--- 總結：共刪除了 12 個檔案。 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 5：切token\n",
        "利用中研院「CKIP Transformers」代替「jieba」進行token處理"
      ],
      "metadata": {
        "id": "vQjZJH5iQPS6"
      },
      "id": "vQjZJH5iQPS6"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U ckip-transformers\n",
        "\n",
        "# import tools: token(WS), tagging(POS), 實體辨識(NER)\n",
        "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
        "\n",
        "# diver initiation(初始化一次之後先關一下)\n",
        "WS_driver = CkipWordSegmenter(model='bert-base')\n",
        "POS_driver = CkipPosTagger(model='bert-base')\n",
        "NER_driver = CkipNerChunker(model='bert-base')\n",
        "\n",
        "# imput data\n",
        "import os, json\n",
        "with open(\"/content/191218_Han_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  f = json.load(f)\n",
        "  myText = \"\"\n",
        "  for i in f:\n",
        "    myText = myText.join(f\"{i} \")\n",
        "print(myText[:5])\n",
        "\n",
        "# run\n",
        "myText_WS = WS_driver(myText)\n",
        "myText_POS = POS_driver(myText_WS) # Changed to pass myText_WS to POS_driver\n",
        "myText_NER = NER_driver(myText)\n",
        "\n",
        "# print result\n",
        "print(\"\\nWS \\n\", myText_WS)\n",
        "print(\"POS \\n\", myText_POS)\n",
        "print(\"NER \\n\", myText_NER)\n",
        "\n",
        "\n",
        "# 引入stopwords dictionary\n",
        "with open(\"/content/stopwords.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  stopwords = f.read().split(\"\\n\")\n",
        "  stopwords = set(stopwords)\n",
        "\n",
        "filtered = []\n",
        "# for i in range(len(myText_WS)):\n",
        "\n"
      ],
      "metadata": {
        "id": "PxitWelbIwkG",
        "outputId": "8d3e3d87-0fa6-4133-cb25-ec6baab5c292",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PxitWelbIwkG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 暫存區"
      ],
      "metadata": {
        "id": "VgxOYM-xc-nA"
      },
      "id": "VgxOYM-xc-nA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question: 能不能把上面操作變成批次處理？（好瘋）"
      ],
      "metadata": {
        "id": "pz4h43Ajetpc"
      },
      "id": "pz4h43Ajetpc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b09fccbc",
      "metadata": {
        "id": "b09fccbc",
        "outputId": "edb739fd-9c4c-4865-8a7a-77b9b661343c",
        "collapsed": true
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '1218.html'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[59], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 從HTHL清資料\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m HTML1218 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1218.html\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(HTML1218, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print(soup)\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1218.html'"
          ]
        }
      ],
      "source": [
        "# 從HTHL清資料（先不用這個東西）\n",
        "\n",
        "HTML1218 = open('1218.html', 'r', encoding='utf-8')\n",
        "soup = BeautifulSoup(HTML1218, 'html.parser')\n",
        "# print(soup)\n",
        "\n",
        "\n",
        "all_spans = soup.find_all('span')\n",
        "content_list = []\n",
        "flag = False\n",
        "\n",
        "for span in all_spans:\n",
        "    text = span.get_text(strip=True)\n",
        "    if text == '逐字稿：':\n",
        "        flag = True\n",
        "    if flag and text:\n",
        "        clean_text = re.sub(r'[\\u3000\\s]+', ' ', text).strip()\n",
        "        print(text)\n",
        "    # if clean_text and clean_text != ' ':\n",
        "    #     content_list.append(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 列印token和tag的結果\n",
        "\n",
        "def pack_ws_pos_sentece(sentence_ws, sentence_pos):\n",
        "    assert len(sentence_ws) == len(sentence_pos)\n",
        "    res = []\n",
        "    for word_ws, word_pos in zip(sentence_ws, sentence_pos):\n",
        "        res.append(f\"{word_ws}({word_pos})\")\n",
        "    return \"\\u3000\".join(res)\n",
        "\n",
        "for myText, ws, pos in zip(myText, myText_WS, myText_POS):\n",
        "  print(myText)\n",
        "  print(pack_ws_pos_sentece(ws, pos))"
      ],
      "metadata": {
        "id": "xQUoP8843-KF",
        "outputId": "7d961b44-a2a5-47b5-8447-777eefa8725b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "id": "xQUoP8843-KF",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'myText' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3027488440.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"\\u3000\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mmyText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyText\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyText_WS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmyText_POS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_ws_pos_sentece\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mws\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'myText' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TQ96_W-t3_W2"
      },
      "id": "TQ96_W-t3_W2",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}