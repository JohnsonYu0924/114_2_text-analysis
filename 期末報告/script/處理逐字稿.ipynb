{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnsonYu0924/114_2_text-analysis/blob/main/%E6%9C%9F%E6%9C%AB%E5%A0%B1%E5%91%8A/script/%E8%99%95%E7%90%86%E9%80%90%E5%AD%97%E7%A8%BF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 0：環境設定"
      ],
      "metadata": {
        "id": "hykoSUUhafeC"
      },
      "id": "hykoSUUhafeC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee54ea0b",
      "metadata": {
        "id": "ee54ea0b",
        "outputId": "8cbedfbe-5ac0-438d-8692-536c7486270d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "import json, re, os, csv\n",
        "import re\n",
        "\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 1：抓取逐字稿"
      ],
      "metadata": {
        "id": "nRLATQu_ak1q"
      },
      "id": "nRLATQu_ak1q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d0ffba9",
      "metadata": {
        "id": "4d0ffba9"
      },
      "outputs": [],
      "source": [
        "# 先寫看看\n",
        "\n",
        "with open('191218_政見發表會.md', 'r', encoding='utf-8') as t:\n",
        "    transcript = t.read()\n",
        "    print(type(transcript)) #class\n",
        "\n",
        "senten = []\n",
        "transcript = re.split(r\"\\n\", transcript)\n",
        "for i in transcript:\n",
        "    if i != \"\":\n",
        "        senten.append(i.strip())\n",
        "\n",
        "candidate = input(\"要查詢的人：\")\n",
        "target_candidate = '#### ' + candidate\n",
        "candidateTranscript = []\n",
        "is_collecting = False\n",
        "\n",
        "for front, later in zip(senten[:-1], senten[1:]):\n",
        "  if front == target_candidate:\n",
        "    is_collecting = not is_collecting\n",
        "  if is_collecting and not re.search(r\"###\", later):\n",
        "    candidateTranscript.append(later)\n",
        "    # print(later)\n",
        "  elif is_collecting and re.search(r\"###\", later):\n",
        "    is_collecting = not is_collecting\n",
        "\n",
        "for i in candidateTranscript:\n",
        "  print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 請Gemini幫我變成def好處理\n",
        "\n",
        "import re\n",
        "import os\n",
        "\n",
        "def candiTrans(filename, candidate):\n",
        "    \"\"\"\n",
        "    從指定的 Markdown 檔案中提取特定候選人的發言內容。\n",
        "\n",
        "    發言區段的判斷邏輯是：\n",
        "    1. 遇到 '#### [候選人姓名]' 標記，開啟/關閉收集狀態 (is_collecting)。\n",
        "    2. 在收集狀態下，直到下一個包含 '###' 的行 (代表另一位候選人發言的開始)\n",
        "       出現為止，收集該區間的所有內容。\n",
        "\n",
        "    Args:\n",
        "        filename (str): 要讀取的檔案名稱。\n",
        "        candidate (str): 要查詢的候選人姓名 (例如: '韓國瑜' 或 '蔡英文')。\n",
        "\n",
        "    Returns:\n",
        "        list: 包含該候選人所有發言句子的列表。\n",
        "    \"\"\"\n",
        "\n",
        "    # 檢查檔案是否存在\n",
        "    if not os.path.exists(filename):\n",
        "        print(f\"錯誤：檔案 '{filename}' 不存在。\")\n",
        "        return []\n",
        "\n",
        "    # 1. 檔案讀取與文本清洗\n",
        "    try:\n",
        "        with open(filename, 'r', encoding='utf-8') as t:\n",
        "            transcript = t.read()\n",
        "    except Exception as e:\n",
        "        print(f\"讀取檔案時發生錯誤: {e}\")\n",
        "        return []\n",
        "\n",
        "    # 將文本按換行符分割並清除空白行和行首尾空白\n",
        "    transcript_lines = re.split(r\"\\n\", transcript)\n",
        "    senten = []\n",
        "    for line in transcript_lines:\n",
        "        stripped_line = line.strip()\n",
        "        if stripped_line != \"\":\n",
        "            senten.append(stripped_line)\n",
        "\n",
        "    # 2. 設定目標分隔符號\n",
        "    target_candidate = '#### ' + candidate\n",
        "    candidateTranscript = []\n",
        "    is_collecting = False\n",
        "\n",
        "    # 3. 遍歷並提取發言內容 (使用 zip 進行前/後配對)\n",
        "    # 注意: 這個邏輯不包含區段開頭和結尾的分隔符號本身。\n",
        "    # 並且，這裡假設所有分隔符號都至少包含 '###'。\n",
        "    for front, later in zip(senten[:-1], senten[1:]):\n",
        "\n",
        "        # 檢查是否遇到目標候選人發言的開頭 (例如: '#### 韓國瑜')\n",
        "        if front == target_candidate:\n",
        "            # 切換狀態：False -> True (開始收集) 或 True -> False (結束收集)\n",
        "            is_collecting = not is_collecting\n",
        "\n",
        "        # 如果正在收集且下一句不是分隔符號\n",
        "        if is_collecting and not re.search(r\"###\", later):\n",
        "            candidateTranscript.append(later)\n",
        "\n",
        "        # 如果正在收集且下一句是分隔符號 (結束收集)\n",
        "        elif is_collecting and re.search(r\"###\", later):\n",
        "            # 切換狀態：True -> False (停止收集)\n",
        "            is_collecting = not is_collecting\n",
        "\n",
        "    return candidateTranscript"
      ],
      "metadata": {
        "id": "1e8mA51kTdUm"
      },
      "id": "1e8mA51kTdUm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 調用function\n",
        "\n",
        "file_name = input(\"請輸入檔案名稱：\")\n",
        "file_name = file_name + '.md'\n",
        "\n",
        "candidate_name = input(\"請輸入要查詢的人：\")\n",
        "\n",
        "result_list = candiTrans(file_name, candidate_name)\n",
        "\n",
        "if result_list:\n",
        "    for item in result_list:\n",
        "        print(item)\n",
        "else:\n",
        "    print(\"未找到發言內容。\")\n"
      ],
      "metadata": {
        "id": "IRTu8mkcusR0"
      },
      "id": "IRTu8mkcusR0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 3：導出為.json檔"
      ],
      "metadata": {
        "id": "IE7qC8rDeEGH"
      },
      "id": "IE7qC8rDeEGH"
    },
    {
      "cell_type": "code",
      "source": [
        "jsonDataName = input('輸入檔名（eg. 191218_Tsai/Han/Soong): ')\n",
        "jsonDataName = jsonDataName + \".json\"\n",
        "\n",
        "with open(jsonDataName, 'w', encoding='utf-8') as d:\n",
        "  json.dump(result_list, d, ensure_ascii=False, indent=4)\n",
        "\n",
        "#重複做把所有.md檔整理成.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtTymFBCej7S",
        "outputId": "10df2c20-5f4f-4d51-9544-f1a95af23ca3"
      },
      "id": "DtTymFBCej7S",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "輸入檔名（eg. 191218_Tsai/Han/Soong): 191229_Soong\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 4：清理逐字稿"
      ],
      "metadata": {
        "id": "i8i6gPHza_Kj"
      },
      "id": "i8i6gPHza_Kj"
    },
    {
      "cell_type": "code",
      "source": [
        "def basicCleaner(file_name):\n",
        "  # file_name = '191227_Han.json'\n",
        "  file_name_w = re.sub(r\"\\.json\", \"\", file_name)\n",
        "\n",
        "  with open(file_name, 'r', encoding='utf-8') as f:\n",
        "    transcript = json.load(f)\n",
        "    print(transcript)\n",
        "\n",
        "  cleanTrans = []\n",
        "  for i in transcript:\n",
        "    i = re.sub(r\"https://\\S+\", \" \", i)\n",
        "    i = re.sub(r\"[!\\[\\]:「」『』《》〈〉（）\\(\\)，、：；。\\.]\", \" \", i)\n",
        "    # i = re.sub(r\"\\d+\", \"\", i) #可以考慮要不要處理數字，跟情緒沒什麼關係\n",
        "    i = i.strip().lower()\n",
        "    cleanTrans.append(i)\n",
        "\n",
        "  for i in cleanTrans:\n",
        "    print(i)\n",
        "\n",
        "  file_name_w = file_name_w + '_cleaned.json'\n",
        "  with open(file_name_w, 'w', encoding='utf-8') as f:\n",
        "    json.dump(cleanTrans, f, ensure_ascii=False, indent=4)\n"
      ],
      "metadata": {
        "id": "ecHWGnxyaduh"
      },
      "id": "ecHWGnxyaduh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 調用function\n",
        "\n",
        "# fileName = input(\"請輸入檔案名稱（含.json）：\")\n",
        "# basicCleaner(fileName)\n",
        "\n",
        "# 批次處理\n",
        "path = \"/content/\"\n",
        "print(os.getcwd())\n",
        "dirList = [i for i in os.listdir(path) if i.endswith(\".json\")]\n",
        "if len(dirList) == 0:\n",
        "  print(\"There's no '.json' data.\")\n",
        "else:\n",
        "  print(dirList)\n",
        "\n",
        "for i in dirList:\n",
        "  basicCleaner(i)"
      ],
      "metadata": {
        "id": "6s-DytCnY2Cx"
      },
      "id": "6s-DytCnY2Cx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#刪除資料\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# ⚠️ 請將這裡替換成您要清理的資料夾路徑\n",
        "folder_path = '/content'\n",
        "\n",
        "# 定義要刪除的檔名模式\n",
        "patterns_to_delete = ['*_cleaned.json', '*_cleaned_cleaned.json']\n",
        "\n",
        "# 遍歷所有模式並刪除檔案\n",
        "deleted_count = 0\n",
        "for pattern in patterns_to_delete:\n",
        "    # 組合完整的搜索路徑\n",
        "    search_path = os.path.join(folder_path, pattern)\n",
        "\n",
        "    # glob.glob() 找到所有符合模式的檔案路徑\n",
        "    for file_path in glob.glob(search_path):\n",
        "        try:\n",
        "            # 刪除檔案\n",
        "            os.remove(file_path)\n",
        "            print(f\"✅ 成功刪除: {file_path}\")\n",
        "            deleted_count += 1\n",
        "        except OSError as e:\n",
        "            # 處理刪除失敗的情況（例如權限不足）\n",
        "            print(f\"❌ 錯誤：無法刪除 {file_path}. {e}\")\n",
        "\n",
        "print(f\"\\n--- 總結：共刪除了 {deleted_count} 個檔案。 ---\")"
      ],
      "metadata": {
        "id": "DbgJHu4fcAsU",
        "outputId": "a489e560-9567-4549-b62b-ff6404af4acd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DbgJHu4fcAsU",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ 成功刪除: /content/191227_Soong_cleaned.json\n",
            "✅ 成功刪除: /content/191218_Tsai_cleaned.json\n",
            "✅ 成功刪除: /content/191227_Han_cleaned.json\n",
            "✅ 成功刪除: /content/191229_Han_cleaned.json\n",
            "✅ 成功刪除: /content/191229_Soong_cleaned.json\n",
            "✅ 成功刪除: /content/191225_Han_cleaned.json\n",
            "✅ 成功刪除: /content/191227_Tsai_cleaned.json\n",
            "✅ 成功刪除: /content/191218_Han_cleaned.json\n",
            "✅ 成功刪除: /content/191229_Tsai_cleaned.json\n",
            "✅ 成功刪除: /content/191225_Soong_cleaned.json\n",
            "✅ 成功刪除: /content/191218_Soong_cleaned.json\n",
            "✅ 成功刪除: /content/191225_Tsai_cleaned.json\n",
            "\n",
            "--- 總結：共刪除了 12 個檔案。 ---\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEP 5：切token\n",
        "利用中研院「CKIP Transformers」代替「jieba」進行token處理"
      ],
      "metadata": {
        "id": "vQjZJH5iQPS6"
      },
      "id": "vQjZJH5iQPS6"
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U ckip-transformers\n",
        "\n",
        "# import tools: token(WS), tagging(POS), 實體辨識(NER)\n",
        "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
        "\n",
        "# diver initiation(初始化一次之後先關一下)\n",
        "WS_driver = CkipWordSegmenter(model='bert-base')\n",
        "POS_driver = CkipPosTagger(model='bert-base')\n",
        "NER_driver = CkipNerChunker(model='bert-base')\n",
        "\n",
        "# imput data\n",
        "import os, json\n",
        "with open(\"/content/191218_Han_cleaned.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "  myText = json.load(f)\n",
        "  myText = myText[:5]\n",
        "print(myText)\n",
        "\n",
        "# run\n",
        "myText_WS = WS_driver(myText)\n",
        "myText_POS = POS_driver(myText)\n",
        "myText_NER = NER_driver(myText)\n",
        "\n",
        "# print result\n",
        "print(\"\\nWS \\n\", myText_WS)\n",
        "print(\"POS \\n\", myText_POS)\n",
        "print(\"NER \\n\", myText_NER)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PxitWelbIwkG",
        "outputId": "dd7e7994-677f-4b9f-95e0-6c887fc8b1cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "PxitWelbIwkG",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['謝謝主持人李進勇主委 蔡英文蔡總統 宋楚瑜宋主席 全國各位所有的台灣同胞 各位海內外關切中華民國第 15 任總統選舉的所有的好朋友大家晚安 大家好！！', '首先非常感謝大家這麼長的一段時間 對韓國瑜的愛護以及支持！更要感謝高雄市 280 萬市民朋友選我為高雄市長 一路陪著我走上這一段神奇的旅程 讓我有這個機會跟大家一起來做出對中華民國未來的改變 以及我們共同來創造未來', '今天是公辦政見發表 所有的政見國防 內政 外交 交通 青年人的未來 毒品治安等等非常非常的多 國家要做的事情非常多 但是這麼多事情之前 有一個最重要的就是我們到底是什麼國家？我們現在到底在辯論什麼？我們是中華民國第 15 任總統的政見發表 還是 這個國家 的政見發表？蔡英文總統擔任總統三年半以來 從來沒有對國家講得清楚 前三年用 這個國家 來代替 所以今天我們政見發表是不是應該改名為 這個國家 政見發表會 還是中華民國總統的政見發表會？', '沒有國就不會有家 我長期一直問蔡英文總統到底要帶兩千三百萬同胞何去何從？蔡英文總統身為中華民國總統 享受到中華民國憲法授予的一切權力跟待遇 但是都是用 這個國家 來稱呼 所以請問蔡英文總統如果不把國家定位講清楚 我們後面的 我們政見說明有什麼意義？', '蔡英文總統眼前有三條路去選擇 我們看的非常清楚 第一 你根本就不愛中華民國 第二 你不要九二共識 我們國民黨的立場非常清楚 在中華民國完整的國格現任體制之下 發展兩岸的交流 是九二共識 你一定要扣一個帽子 叫做我們認同一國兩制 第三 我覺得蔡英文總統更講不清楚 而且完全對不起長期支持台灣獨立的朋友 很多台獨的朋友把選票投給蔡英文總統 希望台灣獨立 但是你完全讓他們失望 所以蔡英文總統必須有責任告訴我們台灣同胞 未來兩千三百萬民眾 到底何去何從？']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tokenization: 100%|██████████| 5/5 [00:00<00:00, 1952.66it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.43s/it]\n",
            "Tokenization: 100%|██████████| 5/5 [00:00<00:00, 3290.17it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:08<00:00,  8.71s/it]\n",
            "Tokenization: 100%|██████████| 5/5 [00:00<00:00, 2424.73it/s]\n",
            "Inference: 100%|██████████| 1/1 [00:03<00:00,  3.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "WS \n",
            " [['謝謝', '主持人', '李進勇', '主委', ' ', '蔡英文', '蔡', '總統', ' ', '宋楚瑜', '宋', '主席', ' ', '全', '國', '各位', '所有', '的', '台灣', '同胞', ' ', '各位', '海內外', '關切', '中華民國', '第 15 ', '任', '總統', '選舉', '的', '所有', '的', '好朋友', '大家', '晚安', ' ', '大家', '好', '！', '！'], ['首先', '非常', '感謝', '大家', '這麼', '長', '的', '一', '段', '時間', ' ', '對', '韓國瑜', '的', '愛護', '以及', '支持', '！', '更', '要', '感謝', '高雄市', ' 280', ' ', '萬', '市民', '朋友', '選', '我', '為', '高雄', '市長', ' ', '一路', '陪', '著', '我', '走上', '這', '一', '段', '神奇', '的', '旅程', ' ', '讓', '我', '有', '這', '個', '機會', '跟', '大家', '一起', '來', '做出', '對', '中華民國', '未來', '的', '改變', ' ', '以及', '我們', '共同', '來', '創造', '未來'], ['今天', '是', '公辦', '政見', '發表', ' ', '所有', '的', '政見', '國防', ' ', '內政', ' ', '外交', ' ', '交通', ' ', '青年人', '的', '未來', ' ', '毒品', '治安', '等等', '非常', '非常', '的', '多', ' ', '國家', '要', '做', '的', '事情', '非常', '多', ' ', '但是', '這麼多', '事情', '之前', ' ', '有', '一', '個', '最', '重要', '的', '就', '是', '我們', '到底', '是', '什麼', '國家', '？', '我們', '現在', '到底', '在', '辯論', '什麼', '？', '我們', '是', '中華民國', '第 15', ' ', '任', '總統', '的', '政見', '發表', ' ', '還是', ' ', '這', '個', '國家', ' ', '的', '政見', '發表', '？', '蔡英文', '總統', '擔任', '總統', '三', '年', '半', '以來', ' ', '從來', '沒有', '對', '國家', '講', '得', '清楚', ' ', '前', '三', '年', '用', ' ', '這', '個', '國家', ' ', '來', '代替', ' ', '所以', '今天', '我們', '政見', '發表', '是', '不是', '應該', '改名為', ' ', '這', '個', '國家', ' ', '政見', '發表會', ' ', '還是', '中華民國', '總統', '的', '政見', '發表會', '？'], ['沒有', '國', '就', '不會', '有', '家', ' ', '我', '長期', '一直', '問', '蔡英文', '總統', '到底', '要', '帶', '兩千三百萬', '同胞', '何去何從', '？', '蔡英文', '總統', '身為', '中華民國', '總統', ' ', '享受到', '中華民國', '憲法', '授予', '的', '一切', '權力', '跟', '待遇', ' ', '但是', '都', '是', '用', ' ', '這', '個', '國家', ' ', '來', '稱呼', ' ', '所以', '請問', '蔡英文', '總統', '如果', '不', '把', '國家', '定位', '講', '清楚', ' ', '我們', '後面', '的', ' ', '我們', '政見', '說明', '有', '什麼', '意義', '？'], ['蔡英文', '總統', '眼前', '有', '三', '條', '路', '去', '選擇', ' ', '我們', '看', '的', '非常', '清楚', ' ', '第一', ' ', '你', '根本', '就', '不', '愛', '中華民國', ' ', '第二', ' ', '你', '不', '要', '九二', '共識', ' ', '我們', '國民黨', '的', '立場', '非常', '清楚', ' ', '在', '中華民國', '完整', '的', '國格', '現任', '體制', '之下', ' ', '發展', '兩岸', '的', '交流', ' ', '是', '九二', '共識', ' ', '你', '一定', '要', '扣', '一', '個', '帽子', ' ', '叫做', '我們', '認同', '一國兩制', ' ', '第三', ' ', '我', '覺得', '蔡英文', '總統', '更', '講', '不', '清楚', ' ', '而且', '完全', '對不起', '長期', '支持', '台灣', '獨立', '的', '朋友', ' ', '很多', '台獨', '的', '朋友', '把', '選票', '投給', '蔡英文', '總統', ' ', '希望', '台灣', '獨立', ' ', '但是', '你', '完全', '讓', '他們', '失望', ' ', '所以', '蔡英文', '總統', '必須', '有', '責任', '告訴', '我們', '台灣', '同胞', ' ', '未來', '兩千三百萬', '民眾', ' ', '到底', '何去何從', '？']]\n",
            "POS \n",
            " [['VJ', 'VJ', 'Na', 'Na', 'Na', 'Nb', 'Nb', 'Nb', 'Na', 'VF', 'WHITESPACE', 'Nb', 'Nb', 'Nb', 'Nb', 'Na', 'VH', 'WHITESPACE', 'Nb', 'Nb', 'Nb', 'Nb', 'Na', 'VH', 'WHITESPACE', 'Neqa', 'Na', 'Nh', 'Nf', 'Neqa', 'V_2', 'DE', 'Nc', 'Na', 'Na', 'Na', 'WHITESPACE', 'Nh', 'Nf', 'Nc', 'Ncd', 'Ncd', 'VJ', 'VJ', 'Nc', 'Nc', 'Nc', 'Nc', 'Neu', 'WHITESPACE', 'Neu', 'Neu', 'WHITESPACE', 'VG', 'Na', 'Na', 'Na', 'Na', 'DE', 'Neqa', 'Neqa', 'DE', 'Na', 'Na', 'Na', 'Nh', 'Nc', 'VH', 'VH', 'WHITESPACE', 'Nh', 'Nc', 'VH', 'EXCLAMATIONCATEGORY', 'EXCLAMATIONCATEGORY'], ['D', 'D', 'Dfa', 'Dfa', 'VK', 'VJ', 'Nh', 'Nc', 'Dfa', 'Dfa', 'VH', 'DE', 'Neu', 'Nf', 'Na', 'Ng', 'WHITESPACE', 'P', 'Nb', 'Nb', 'Nb', 'DE', 'VC', 'VC', 'Caa', 'Caa', 'VC', 'VC', 'EXCLAMATIONCATEGORY', 'Dfa', 'D', 'VK', 'VJ', 'Nc', 'Nc', 'Nc', 'WHITESPACE', 'Neu', 'Neu', 'FW', 'WHITESPACE', 'Neu', 'Na', 'Na', 'Na', 'Na', 'VC', 'Nh', 'P', 'Nc', 'Nc', 'Na', 'VH', 'WHITESPACE', 'D', 'Nf', 'VC', 'Di', 'Nh', 'VCL', 'Di', 'Nep', 'Neu', 'Nf', 'VH', 'VH', 'DE', 'Na', 'Na', 'WHITESPACE', 'VL', 'Nh', 'V_2', 'Nep', 'Nf', 'Na', 'D', 'P', 'Nh', 'Nc', 'D', 'D', 'D', 'VC', 'VC', 'P', 'Nc', 'Nc', 'Nc', 'Nc', 'Nd', 'Ng', 'DE', 'VC', 'VF', 'WHITESPACE', 'Caa', 'Caa', 'Nh', 'Nh', 'A', 'A', 'D', 'VC', 'VC', 'Nd', 'D'], ['Nd', 'Nf', 'SHI', 'VC', 'VC', 'Na', 'VE', 'VC', 'VC', 'WHITESPACE', 'Neqa', 'V_2', 'DE', 'Na', 'Nv', 'Na', 'Nv', 'WHITESPACE', 'Na', 'Na', 'WHITESPACE', 'Na', 'VJ', 'WHITESPACE', 'Na', 'VC', 'WHITESPACE', 'Na', 'Na', 'Na', 'DE', 'Nd', 'Ng', 'WHITESPACE', 'Na', 'Na', 'Na', 'Na', 'Cab', 'Cab', 'Dfa', 'Dfa', 'Dfa', 'Dfa', 'DE', 'VH', 'WHITESPACE', 'Na', 'Nc', 'D', 'VC', 'DE', 'Na', 'Na', 'Dfa', 'VH', 'VH', 'WHITESPACE', 'Cbb', 'SHI', 'Neqa', 'Neqa', 'VH', 'Na', 'Na', 'Ng', 'Ng', 'WHITESPACE', 'V_2', 'Neu', 'Nf', 'Dfa', 'VH', 'VC', 'DE', 'D', 'SHI', 'Nh', 'Nh', 'D', 'DE', 'SHI', 'Nep', 'Nep', 'Na', 'Nc', 'QUESTIONCATEGORY', 'Nh', 'Nh', 'Nd', 'P', 'D', 'Ncd', 'D', 'VE', 'VE', 'Nep', 'Nep', 'QUESTIONCATEGORY', 'Nh', 'D', 'SHI', 'Nc', 'Nc', 'Nc', 'Nc', 'Neu', 'WHITESPACE', 'Neu', 'Neu', 'WHITESPACE', 'Nf', 'Na', 'Na', 'DE', 'Na', 'Na', 'VC', 'VC', 'WHITESPACE', 'Caa', 'SHI', 'WHITESPACE', 'Nep', 'Nf', 'Na', 'Nc', 'WHITESPACE', 'DE', 'Na', 'Na', 'VC', 'VC', 'QUESTIONCATEGORY', 'Nb', 'Nb', 'Nb', 'Na', 'Na', 'VG', 'VG', 'Na', 'Na', 'Neu', 'Nf', 'Neqa', 'Ng', 'Ng', 'WHITESPACE', 'D', 'D', 'D', 'D', 'P', 'Na', 'Na', 'VE', 'DE', 'VH', 'VH', 'WHITESPACE', 'Nes', 'Neu', 'Nf', 'VC', 'WHITESPACE', 'Nep', 'Nf', 'Na', 'Na', 'WHITESPACE', 'D', 'VC', 'P', 'WHITESPACE', 'Cbb', 'P', 'Nd', 'Nf', 'Nh', 'Na', 'Na', 'Na', 'VC', 'Na', 'SHI', 'D', 'SHI', 'D', 'D', 'VG', 'VG', 'VG', 'WHITESPACE', 'Nep', 'Nf', 'Na', 'Na', 'WHITESPACE', 'Na', 'Na', 'Na', 'Na', 'Na', 'WHITESPACE', 'Caa', 'SHI', 'Nc', 'Nc', 'Nd', 'Nc', 'Na', 'Na', 'DE', 'Na', 'Na', 'Na', 'Na', 'Na', 'QUESTIONCATEGORY'], ['VJ', 'V_2', 'Nc', 'D', 'D', 'D', 'V_2', 'Nf', 'WHITESPACE', 'Nh', 'Na', 'Na', 'D', 'D', 'VE', 'Nb', 'Nb', 'Nb', 'Na', 'Na', 'D', 'Ncd', 'D', 'VC', 'Neu', 'Neu', 'Neu', 'Neu', 'Neu', 'Na', 'Na', 'VA', 'D', 'VA', 'VJ', 'QUESTIONCATEGORY', 'Nb', 'Nb', 'Nb', 'Na', 'Na', 'VG', 'VG', 'Nc', 'Nc', 'Nd', 'Na', 'Na', 'VH', 'WHITESPACE', 'VJ', 'VJ', 'P', 'Nc', 'Nc', 'Nd', 'Na', 'Na', 'Na', 'VD', 'VD', 'DE', 'Neqa', 'Neqa', 'Na', 'Na', 'Caa', 'Na', 'VC', 'WHITESPACE', 'Cbb', 'SHI', 'D', 'SHI', 'P', 'WHITESPACE', 'Nep', 'Nf', 'Na', 'Nc', 'WHITESPACE', 'D', 'VG', 'VG', 'WHITESPACE', 'Cbb', 'Cbb', 'VE', 'VE', 'Nb', 'Nb', 'Nb', 'Na', 'Na', 'Cbb', 'Cbb', 'D', 'P', 'Na', 'Na', 'Na', 'Na', 'VE', 'VH', 'VH', 'WHITESPACE', 'Nh', 'Na', 'Ncd', 'Ncd', 'DE', 'WHITESPACE', 'Nh', 'Na', 'Na', 'Na', 'Na', 'D', 'V_2', 'Nep', 'Nep', 'Na', 'Na', 'QUESTIONCATEGORY'], ['Nb', 'Nb', 'Nb', 'Na', 'Na', 'Nc', 'Ncd', 'V_2', 'Neu', 'Nf', 'Na', 'D', 'VC', 'VC', 'WHITESPACE', 'Nh', 'Na', 'VC', 'DE', 'Dfa', 'Dfa', 'VH', 'VH', 'WHITESPACE', 'Neu', 'Neu', 'WHITESPACE', 'Nh', 'D', 'D', 'D', 'D', 'VL', 'Nc', 'Nc', 'Nd', 'Na', 'WHITESPACE', 'Neu', 'Neu', 'WHITESPACE', 'Nh', 'D', 'VC', 'Nd', 'Neu', 'Na', 'Na', 'WHITESPACE', 'Nh', 'Na', 'Nb', 'Nb', 'Na', 'DE', 'Na', 'Na', 'Dfa', 'Dfa', 'VH', 'VH', 'WHITESPACE', 'P', 'Nc', 'Nc', 'Nd', 'Nc', 'VH', 'Neqa', 'DE', 'Na', 'Na', 'A', 'Nf', 'Na', 'Na', 'Ng', 'Ng', 'WHITESPACE', 'VC', 'VC', 'Nc', 'Nc', 'DE', 'Nv', 'Nv', 'WHITESPACE', 'SHI', 'Nd', 'Neu', 'Na', 'VK', 'WHITESPACE', 'Nh', 'D', 'D', 'D', 'VC', 'Neu', 'Nf', 'Na', 'VH', 'WHITESPACE', 'VG', 'VG', 'Nh', 'Na', 'VJ', 'Nes', 'Na', 'Na', 'Neu', 'Na', 'WHITESPACE', 'Neu', 'Neu', 'WHITESPACE', 'Nh', 'VK', 'VK', 'Nb', 'Na', 'Nb', 'Na', 'Na', 'D', 'VE', 'D', 'VH', 'VH', 'WHITESPACE', 'Cbb', 'Cbb', 'D', 'Neqa', 'VJ', 'D', 'VJ', 'Nd', 'Nd', 'VC', 'VC', 'Nc', 'Na', 'VH', 'VC', 'DE', 'Na', 'Na', 'WHITESPACE', 'Neqa', 'VH', 'Na', 'VH', 'DE', 'Na', 'Na', 'P', 'Na', 'Na', 'VD', 'P', 'Nb', 'Nb', 'Na', 'Na', 'Na', 'WHITESPACE', 'VK', 'VK', 'Nc', 'Na', 'VH', 'VH', 'WHITESPACE', 'Cbb', 'SHI', 'Nh', 'D', 'Neqa', 'VL', 'Nh', 'Na', 'VH', 'VK', 'WHITESPACE', 'Cbb', 'T', 'Nb', 'Nb', 'Nb', 'Na', 'VC', 'D', 'D', 'V_2', 'Na', 'VG', 'VE', 'VE', 'Nh', 'Na', 'Nc', 'Na', 'Na', 'Na', 'WHITESPACE', 'Nd', 'VA', 'Neu', 'Neu', 'Neu', 'Neu', 'Neu', 'Na', 'Na', 'WHITESPACE', 'D', 'DE', 'VA', 'VCL', 'Nes', 'VJ', 'QUESTIONCATEGORY']]\n",
            "NER \n",
            " [[NerToken(word='李進勇', ner='PERSON', idx=(5, 8)), NerToken(word='蔡英文', ner='PERSON', idx=(11, 14)), NerToken(word='蔡', ner='PERSON', idx=(14, 15)), NerToken(word='宋楚瑜', ner='PERSON', idx=(18, 21)), NerToken(word='宋', ner='PERSON', idx=(21, 22)), NerToken(word='台灣', ner='NORP', idx=(32, 34)), NerToken(word='中華民國', ner='GPE', idx=(44, 48)), NerToken(word='第 ', ner='ORDINAL', idx=(48, 50))], [NerToken(word='韓國瑜', ner='PERSON', idx=(18, 21)), NerToken(word='高雄市', ner='GPE', idx=(33, 36)), NerToken(word='280 萬', ner='CARDINAL', idx=(37, 42)), NerToken(word='高雄', ner='GPE', idx=(49, 51)), NerToken(word='中華民國', ner='GPE', idx=(86, 90))], [NerToken(word='今天', ner='DATE', idx=(0, 2)), NerToken(word='中華民國', ner='GPE', idx=(102, 106)), NerToken(word='第 15', ner='ORDINAL', idx=(106, 110)), NerToken(word='蔡英文', ner='PERSON', idx=(134, 137)), NerToken(word='三年半', ner='DATE', idx=(143, 146)), NerToken(word='前三年', ner='DATE', idx=(161, 164)), NerToken(word='今天', ner='DATE', idx=(177, 179)), NerToken(word='中華民國', ner='GPE', idx=(207, 211))], [NerToken(word='蔡英文', ner='PERSON', idx=(15, 18)), NerToken(word='兩千三百萬', ner='CARDINAL', idx=(24, 29)), NerToken(word='蔡英文', ner='PERSON', idx=(36, 39)), NerToken(word='中華民國', ner='GPE', idx=(43, 47)), NerToken(word='中華民國憲法', ner='LAW', idx=(53, 59)), NerToken(word='蔡英文', ner='PERSON', idx=(89, 92))], [NerToken(word='蔡英文', ner='PERSON', idx=(0, 3)), NerToken(word='三', ner='CARDINAL', idx=(8, 9)), NerToken(word='第一', ner='ORDINAL', idx=(24, 26)), NerToken(word='中華民國', ner='GPE', idx=(33, 37)), NerToken(word='第二', ner='ORDINAL', idx=(38, 40)), NerToken(word='九二', ner='CARDINAL', idx=(44, 46)), NerToken(word='國民黨', ner='ORG', idx=(51, 54)), NerToken(word='中華民國', ner='GPE', idx=(63, 67)), NerToken(word='兩', ner='CARDINAL', idx=(81, 82)), NerToken(word='九二', ner='CARDINAL', idx=(88, 90)), NerToken(word='一', ner='CARDINAL', idx=(98, 99)), NerToken(word='一', ner='CARDINAL', idx=(109, 110)), NerToken(word='兩', ner='CARDINAL', idx=(111, 112)), NerToken(word='第三', ner='ORDINAL', idx=(114, 116)), NerToken(word='蔡英文', ner='PERSON', idx=(120, 123)), NerToken(word='台灣', ner='GPE', idx=(142, 144)), NerToken(word='台獨', ner='GPE', idx=(152, 154)), NerToken(word='蔡英文', ner='PERSON', idx=(162, 165)), NerToken(word='台灣', ner='GPE', idx=(170, 172)), NerToken(word='蔡英文', ner='PERSON', idx=(188, 191)), NerToken(word='台灣', ner='GPE', idx=(202, 204)), NerToken(word='兩千三百萬', ner='CARDINAL', idx=(209, 214))]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 暫存區"
      ],
      "metadata": {
        "id": "VgxOYM-xc-nA"
      },
      "id": "VgxOYM-xc-nA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question: 能不能把上面操作變成批次處理？（好瘋）"
      ],
      "metadata": {
        "id": "pz4h43Ajetpc"
      },
      "id": "pz4h43Ajetpc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b09fccbc",
      "metadata": {
        "id": "b09fccbc",
        "outputId": "edb739fd-9c4c-4865-8a7a-77b9b661343c"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '1218.html'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[59], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 從HTHL清資料\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m HTML1218 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1218.html\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(HTML1218, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# print(soup)\u001b[39;00m\n",
            "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1218.html'"
          ]
        }
      ],
      "source": [
        "# 從HTHL清資料（先不用這個東西）\n",
        "\n",
        "HTML1218 = open('1218.html', 'r', encoding='utf-8')\n",
        "soup = BeautifulSoup(HTML1218, 'html.parser')\n",
        "# print(soup)\n",
        "\n",
        "\n",
        "all_spans = soup.find_all('span')\n",
        "content_list = []\n",
        "flag = False\n",
        "\n",
        "for span in all_spans:\n",
        "    text = span.get_text(strip=True)\n",
        "    if text == '逐字稿：':\n",
        "        flag = True\n",
        "    if flag and text:\n",
        "        clean_text = re.sub(r'[\\u3000\\s]+', ' ', text).strip()\n",
        "        print(text)\n",
        "    # if clean_text and clean_text != ' ':\n",
        "    #     content_list.append(clean_text)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}