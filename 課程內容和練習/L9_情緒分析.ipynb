{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JohnsonYu0924/114_2_text-analysis/blob/main/L9_%E6%83%85%E7%B7%92%E5%88%86%E6%9E%90.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 課程概要\n",
        "\n",
        "## 情緒分析\n",
        "1. 字典法\n",
        "2. 監督式分析：已經有先標注好/訓練好的文本\n",
        "3. 非監督式分析法\n",
        "\n",
        "## 訓練集與測試集\n",
        "1. 80：訓練集（Training Set）\n",
        "2. 20：測試集（Test Set）\n",
        "\n",
        "## 模型訓練與評估流程\n",
        "1. 建立：\\\n",
        "對訓練集和測試集進行前處理和向量化。\n",
        "2. 建立y標籤列表（pos | neg）\n",
        "3. 訓練模型\n",
        "4. 在測試資料上跑模型\n",
        "5. 評估狀況\n",
        "\n",
        "## 好工具：VADER"
      ],
      "metadata": {
        "id": "faZ5svZjnoBQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scikit-Learn：逐步示範 fit 與 predict"
      ],
      "metadata": {
        "id": "uS8OeJ9msWNV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RjCQnGphsCMj",
        "outputId": "c1ae2cd0-e4ea-41c6-bdd2-1d61e47b49d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:23: SyntaxWarning: invalid escape sequence '\\s'\n",
            "<>:23: SyntaxWarning: invalid escape sequence '\\s'\n",
            "/tmp/ipython-input-2674138112.py:23: SyntaxWarning: invalid escape sequence '\\s'\n",
            "  doc = re.sub(\"\\s+\", \" \", doc)\n"
          ]
        }
      ],
      "source": [
        "## Slide 2 — Separate Steps Version\n",
        "## Scikit-Learn：逐步示範向量化與模型訓練\n",
        "\n",
        "mydir = \"/content/dissents/\"\n",
        "\n",
        "from nltk.corpus.reader.plaintext import CategorizedPlaintextCorpusReader\n",
        "import re\n",
        "\n",
        "# 定義文件與類別（pos / neg）\n",
        "documentPattern = r'[A-z0-9\\s.]+\\.txt'\n",
        "categoryPattern = r'.*(pos|neg).*'\n",
        "\n",
        "myCorpus2 = CategorizedPlaintextCorpusReader(\n",
        "    mydir,\n",
        "    documentPattern,\n",
        "    cat_pattern=categoryPattern\n",
        ")\n",
        "\n",
        "# 將所有文本轉為字串列表\n",
        "strCorpus = []\n",
        "for file in myCorpus2.fileids():\n",
        "    doc = myCorpus2.raw(file)\n",
        "    doc = re.sub(\"\\s+\", \" \", doc)\n",
        "    strCorpus.append(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 建立 X：詞頻矩陣 + One-Hot Encoding"
      ],
      "metadata": {
        "id": "9m4mn4W0sff-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import Binarizer\n",
        "\n",
        "# 建立詞頻矩陣 (Count Matrix)\n",
        "freq = CountVectorizer()\n",
        "corpus = freq.fit_transform(strCorpus)\n",
        "\n",
        "# One-hot encoding（將 count 轉為 0/1）\n",
        "onehot = Binarizer()\n",
        "documents = onehot.fit_transform(corpus.toarray())\n",
        "documents\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qX-dD520sgVP",
        "outputId": "1ad30565-31a3-4480-8a46-79c28fee90ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [0, 0, 0, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 1, 0, 0],\n",
              "       [0, 0, 0, ..., 1, 0, 0],\n",
              "       [0, 0, 0, ..., 1, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 建立 y：文件標籤"
      ],
      "metadata": {
        "id": "Ik9D6-52si7i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "labels = []\n",
        "for doc in myCorpus2.fileids():\n",
        "    labels.append(myCorpus2.categories(doc))\n",
        "\n",
        "labels = np.array(labels).ravel()\n",
        "print(labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oCZ1Lm9sjpv",
        "outputId": "af92cea3-c42a-421e-c2a9-4761ee873807"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['neg' 'neg' 'neg' 'pos' 'pos' 'pos']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 訓練 Naive Bayes 模型\n",
        "\n",
        "- 依據每次的事件，改變原本發生的機率。\n",
        "- alpha=0.1（Laplace smoothing）\n",
        "  - 避免某些詞在某類別中出現次數為 0 時產生錯誤。\n",
        "  - 越小 → 越接近原始數據\n",
        "  - 越大 → 模型比較「平滑」\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ybCYioRNsmHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model1 = MultinomialNB(alpha=0.1, class_prior=[0.4, 0.6])\n",
        "model1.fit(documents, labels)\n",
        "#documents 是 CountVectorizer 或 Binarizer 做好的詞頻矩陣\n",
        "#labels 是 ['neg', 'pos', ...] 的標籤\n",
        "\n",
        "\n",
        "# 使用相同資料預測（實務中應使用測試集）\n",
        "model1.predict(documents)\n",
        "\n",
        "\n",
        "# 這行讓模型：\n",
        "# 讀文件的詞頻向量\n",
        "# 計算每個類別的 posterior probability\n",
        "# 選擇最大那一類（pos 或 neg）\n",
        "\n",
        "# posterior probability（後驗機率）是什麼\n",
        "# posterior probability = 模型看到文章之後，推測該文章屬於某個類別的機率\n",
        "# 後驗 = 先驗 × 文件中每個詞提供的證據"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vMc3OjcsnrD",
        "outputId": "a0a6e8f6-5004-40c8-918a-833e4bf04f69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['neg', 'neg', 'neg', 'pos', 'pos', 'pos'], dtype='<U3')"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 其他方法：Pipeline（結合向量化＋模型）"
      ],
      "metadata": {
        "id": "yZ6KCjU0suqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model2 = Pipeline([\n",
        "    ('vectorizerTfIdf', TfidfVectorizer()),  # TfidfVectorizer() 把文本轉成 TF-IDF 特徵矩陣。\n",
        "    ('bayes', MultinomialNB())  #MultinomialNB(): 讀取 TFIDF 特徵並訓練 Naive Bayes 分類器。\n",
        "])\n",
        "\n",
        "model2.steps\n",
        "model2.named_steps[\"bayes\"]\n",
        "\n",
        "# 檢查 Pipeline 裡面的內容，用來：\n",
        "# 看模型裡到底有哪些步驟，會輸出我們定義的上兩個\n",
        "# 取出特定步驟（像是取出 Naive Bayes 模型本體），也就是第二個步驟\n",
        "\n",
        "\n",
        "model2.fit(strCorpus, labels) # fit（訓練）\n",
        "model2.predict(strCorpus) # predict（用同一資料預測）\n",
        "model2.score(strCorpus, labels) # score（計算準確度）\n",
        "\n",
        "\n",
        "# 1.0（100% 準確率）"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ib32g0aOsxpu",
        "outputId": "a05478d8-4978-42ed-87ed-f314924cdb2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Iqual Ch. 10"
      ],
      "metadata": {
        "id": "zeDeuFaMszvV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 1: 讀取與前處理"
      ],
      "metadata": {
        "id": "1kx1W5j7xM1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, re, string\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "snowball = SnowballStemmer(\"english\")\n",
        "wordnet = WordNetLemmatizer()\n",
        "\n",
        "dissents = []\n",
        "dirlist = os.listdir(mydir)\n",
        "\n",
        "# 將每篇 dissent 前處理\n",
        "for entry in dirlist:\n",
        "    infile = mydir + entry\n",
        "\n",
        "    # 跳過系統資料夾（像 .ipynb_checkpoints）\n",
        "    if os.path.isdir(infile):\n",
        "        continue\n",
        "\n",
        "    # 跳過非 txt 檔案\n",
        "    if not entry.endswith(\".txt\"):\n",
        "        continue\n",
        "\n",
        "    txt = open(infile).read()\n",
        "\n",
        "    txt = re.sub(\"[\\\\s]+\", \" \", txt).lower()\n",
        "    tokens = word_tokenize(txt)\n",
        "\n",
        "    cleaned = []\n",
        "    for w in tokens:\n",
        "        if re.search(\"^[a-z]+$\", w):\n",
        "            w = wordnet.lemmatize(w)\n",
        "            w = snowball.stem(w)\n",
        "            cleaned.append(w)\n",
        "\n",
        "    dissents.append(cleaned)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeKXymJDs0hb",
        "outputId": "358821a3-d238-4c24-fefb-272058f523da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 2: 建立 Document-Term Matrix（手動版）"
      ],
      "metadata": {
        "id": "bHIdAttds4gQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "# 建立 vocabulary\n",
        "vocab = set()\n",
        "for dissent in dissents:\n",
        "    vocab.update(dissent)\n",
        "\n",
        "print(len(vocab))  # 特徵數目\n",
        "\n",
        "# 計算每個文件的詞頻\n",
        "dissentFreqs = []\n",
        "for dissent in dissents:\n",
        "    tf = Counter(dissent) # 計算一個 list 裡每個詞出現的次數\n",
        "    row = [tf[token] if token in tf else 0 for token in vocab]\n",
        "    dissentFreqs.append(row)\n",
        "\n",
        "freqMat = np.matrix(dissentFreqs) #最後要把各文件的詞頻組成矩陣（matrix）\n",
        "print(freqMat)\n",
        "\n",
        "\n",
        "# 492 字彙表大小: 一共出現了 492 個不同的詞\n",
        "#"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YqW6fatPs5MH",
        "outputId": "35fda529-7c01-4469-a3fe-cf655c287ffb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "492\n",
            "[[ 1  0 11 ...  0  2  1]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  1  0  0]\n",
            " [ 0  0  0 ...  0  0  0]\n",
            " [ 0  1  0 ...  0  0  0]\n",
            " [ 0  0  0 ...  0  0  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 範例：處理電影評論"
      ],
      "metadata": {
        "id": "NEhKgkjBs9nO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 1: 訓練 / 測試資料讀取"
      ],
      "metadata": {
        "id": "i5_93gcD4x8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "base = \"Week9/\"\n",
        "folders = [\n",
        "    \"train/pos/\", \"train/neg/\",\n",
        "    \"test/pos/\", \"test/neg/\"\n",
        "]\n",
        "\n",
        "# 建立資料夾\n",
        "for f in folders:\n",
        "    os.makedirs(base + f, exist_ok=True)\n",
        "\n",
        "# 一些簡短示例影評\n",
        "pos_reviews = [\n",
        "    \"I absolutely loved this movie. The acting was fantastic!\",\n",
        "    \"A touching and beautifully filmed story.\",\n",
        "    \"Great characters and a powerful ending. Highly recommend.\",\n",
        "    \"This movie made me smile the entire time.\",\n",
        "    \"A masterpiece. Wonderful soundtrack and stunning visuals.\",\n",
        "]\n",
        "\n",
        "neg_reviews = [\n",
        "    \"This movie was boring and way too long.\",\n",
        "    \"Terrible acting and the plot made no sense.\",\n",
        "    \"I regret watching this. Complete waste of time.\",\n",
        "    \"The script was weak and the pacing was awful.\",\n",
        "    \"One of the worst movies I have seen this year.\",\n",
        "]\n",
        "\n",
        "# 為 train/test 隨機分配資料（各類 2–3 則）\n",
        "def save_reviews(reviews, folder, prefix):\n",
        "    for i, text in enumerate(reviews):\n",
        "        with open(base + folder + f\"{prefix}_{i}.txt\", \"w\") as f:\n",
        "            f.write(text)\n",
        "\n",
        "# train\n",
        "save_reviews(pos_reviews[:3], \"train/pos/\", \"pos\")\n",
        "save_reviews(neg_reviews[:3], \"train/neg/\", \"neg\")\n",
        "\n",
        "# test\n",
        "save_reviews(pos_reviews[3:], \"test/pos/\", \"pos\")\n",
        "save_reviews(neg_reviews[3:], \"test/neg/\", \"neg\")\n",
        "\n",
        "print(\"Small IMDB-style dataset created!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k5SwS37feFSn",
        "outputId": "54f024ae-d45e-4a4f-f13a-e7cfac814cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Small IMDB-style dataset created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCpoAhlLe_st",
        "outputId": "ad0e7b6a-3d76-4beb-8273-ca9643c1e80f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.4.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading Unidecode-1.4.0-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# y軸標籤\n",
        "from unidecode import unidecode\n",
        "\n",
        "trainText, testText = [], []\n",
        "\n",
        "print(\"Reading training data\")\n",
        "for folder, label in [(\"train/pos/\", 0), (\"train/neg/\", 1)]: # 讀取訓練資料\n",
        "    for file in os.listdir(\"/content/Week9/\" + folder):\n",
        "        if file.endswith(\".txt\"):\n",
        "            txt = open(\"/content/Week9/\" + folder + file).read()\n",
        "            trainText.append(unidecode(txt))\n",
        "\n",
        "# pos / neg 計數\n",
        "num_posTrain = len(os.listdir(\"/content/Week9/train/pos/\"))\n",
        "num_negTrain = len(os.listdir(\"/content/Week9/train/neg/\"))\n",
        "\n",
        "# 建立 targetTrain\n",
        "targetTrain = [0] * num_posTrain + [1] * num_negTrain\n",
        "\n",
        "print(\"Reading test data\")\n",
        "for folder, label in [(\"test/pos/\", 0), (\"test/neg/\", 1)]: # 把 test/pos/ 和 test/neg/ 裡的每一個 .txt 文件讀進 testText\n",
        "    for file in os.listdir(\"/content/Week9/\" + folder):\n",
        "        if file.endswith(\".txt\"):\n",
        "            txt = open(\"/content/Week9/\" + folder + file).read()\n",
        "            testText.append(unidecode(txt))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlS9aLSfs-PP",
        "outputId": "30691f37-40c5-4ab2-ecfd-db1b51c5f15c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading training data\n",
            "Reading test data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 2: BOW 前處理函式"
      ],
      "metadata": {
        "id": "oz8YgNmjtG6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def BoW(text):\n",
        "    tokenized = [word_tokenize(doc) for doc in text]\n",
        "\n",
        "    # 移除標點符號\n",
        "    regex = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "    cleaned_docs = []\n",
        "    for review in tokenized:\n",
        "        cleaned = [regex.sub('', w) for w in review if regex.sub('', w)]\n",
        "        cleaned_docs.append(cleaned)\n",
        "\n",
        "    # Stemming\n",
        "    porter = PorterStemmer()\n",
        "    final_docs = [\" \".join(porter.stem(w) for w in doc) for doc in cleaned_docs]\n",
        "\n",
        "    return final_docs"
      ],
      "metadata": {
        "id": "sp3zo51ctH13"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 3: 向量化、建立 y、訓練模型"
      ],
      "metadata": {
        "id": "M26UItuItLNB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "train_docs = BoW(trainText) # 前處理文字 → 轉成 TF-IDF 特徵\n",
        "tfidf = TfidfVectorizer(min_df = 1) #TF-IDF 向量化（min_df = 1: 至少出現1次才存進來）\n",
        "trainData = tfidf.fit_transform(train_docs) #fit()：建立詞彙表與 TF-IDF 權重 #transform()：把文本變成數字向量\n",
        "\n",
        "test_docs = BoW(testText)\n",
        "testData = tfidf.transform(test_docs)\n",
        "\n",
        "# 建立標籤\n",
        "targetTrain = [0]*num_posTrain + [1]*(len(trainText)-num_posTrain) # [0] * num_posTrain 是「把 0 重複 num_posTrain 次」。 [0] * 5 會是 [0, 0, 0, 0, 0]\n",
        "targetTest = [0]*sum(1 for _ in testText[:len(testText)//2]) + [1]*(len(testText)-len(testText)//2)\n",
        "\n",
        "# [0]*sum(1 for _ in testText[:len(testText)//2]) 前面一半的元素\n",
        "# [1]*(len(testText)-len(testText)//2) 後半段元素\n"
      ],
      "metadata": {
        "id": "FPuszm7OtLyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 4-1: Naive Bayes 評估模型"
      ],
      "metadata": {
        "id": "6SsE1oa0tQ8L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import GaussianNB #匯入 Gaussian Naive Bayes 模型\n",
        "\n",
        "gnb = GaussianNB() #創建一個空的分類模型\n",
        "gnb.fit(trainData.toarray(), targetTrain) #用訓練資料訓練模型。 toarray: 因為 GaussianNB 不接受 sparse matrix，需要轉成 numpy array。\n",
        "pred = gnb.predict(testData.toarray()) #用訓練好的模型預測 test 資料\n",
        "\n",
        "print(\"Mislabeled test points:\", (np.array(targetTest) != pred).sum())\n",
        "\n",
        "# 比較 targetTest 與 pred 是否相同\n",
        "# 如果 預測 ≠ 真實 → True（代表錯誤）\n",
        "# 如果 預測 = 真實 → False（代表正確）\n",
        "# 因為我們是用 != 所以當兩者不同，會被標記為 T 這樣我們也才可以算 \"mislabeled test points\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY9uOwQJtNNS",
        "outputId": "d47fda4d-fd0a-4c1e-8917-71e9fe0e18ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mislabeled test points: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## STEP 4-2: Support Vector Machine（SVM） 模型"
      ],
      "metadata": {
        "id": "L88zdm6xtUvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(targetTrain)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zzW7HcHtZyi",
        "outputId": "90a93cb9-221b-4aac-a815-5cb13e102ed1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 1, 1, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "\n",
        "clf = svm.SVC() #建立一個 SVM 類別器\n",
        "pred2 = clf.fit(trainData.toarray(), targetTrain).predict(testData.toarray())\n",
        "\n",
        "# fit(trainData.toarray(), targetTrain): 訓練 SVM 模型\n",
        "# .predict(testData.todense()): 使用 SVM 預測 test data\n",
        "# pred2 = 把預測結果存起來\n",
        "\n",
        "print(\"Mislabeled points:\", (np.array(targetTest) != pred2).sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KD_KWG2StVXO",
        "outputId": "1a88e93d-cce8-428f-e17d-9669b9d4fd77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mislabeled points: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 字典法Dictionary Method\n"
      ],
      "metadata": {
        "id": "rXHmrRfFtZOv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Bing Liu"
      ],
      "metadata": {
        "id": "8jo5p8gr7MNF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('opinion_lexicon')\n",
        "# NLTK 內建的情緒字典:\n",
        "# positive words（例如：good, excellent, amazing, beautiful…\n",
        "# negative words（例如：bad, terrible, ugly, hate…）\n",
        "\n",
        "from nltk.corpus import opinion_lexicon\n",
        "\n",
        "pos_list = set(opinion_lexicon.positive()) #讀取正向字典\n",
        "neg_list = set(opinion_lexicon.negative()) #讀取負向字典\n",
        "\n",
        "def sentiment1(text):\n",
        "    score = 0 #初始情緒分數為 0。\n",
        "    words = [w.lower() for w in word_tokenize(text)]\n",
        "    for w in words:\n",
        "        if w in pos_list:\n",
        "            score += 1\n",
        "        elif w in neg_list:\n",
        "            score -= 1\n",
        "    return score\n",
        "\n",
        "docSenti1 = [sentiment1(doc) for doc in strCorpus] #對 strCorpus 裡的每篇文章跑 sentiment1()\n",
        "print(docSenti1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cUBm3LQtZ4B",
        "outputId": "c5c81c29-be1d-4743-94a3-66d40dccae07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package opinion_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/opinion_lexicon.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-6, -3, -4, 20, -2, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. VADER（另一種情感分析字典）"
      ],
      "metadata": {
        "id": "54jrOtZJtdAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install vaderSentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjEQWf6T1Zaj",
        "outputId": "30a74990-302f-4240-cb63-609ff881fa34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting vaderSentiment\n",
            "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from vaderSentiment) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->vaderSentiment) (2025.11.12)\n",
            "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vaderSentiment\n",
            "Successfully installed vaderSentiment-3.3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "analyzer = SentimentIntensityAnalyzer()\n",
        "\n",
        "def sentiment_vader(text):\n",
        "    return analyzer.polarity_scores(text)\n",
        "\n",
        "docSenti2 = [sentiment_vader(doc) for doc in strCorpus]\n",
        "docSenti2\n",
        "\n",
        "# 若只想取某一個情緒指標，例如 negative\n",
        "docSenti3 = [sentiment_vader(doc)[\"neg\"] for doc in strCorpus]\n",
        "print(docSenti3)\n",
        "\n",
        "\n",
        "# 用的是 neg\n",
        "# 0.195: 約 19.5% 的字詞情緒上是負面\n",
        "# 0.039: 幾乎沒有負面情緒"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw85Qm1FtdmP",
        "outputId": "ccd44aef-4777-42b3-8994-46ab402e8bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.195, 0.267, 0.139, 0.039, 0.196, 0.038]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> 1. 分別針對整份文件的正負情感加總指數\n",
        "> 2. 分別針對幾份文件的正負情感比例"
      ],
      "metadata": {
        "id": "2LmwuEFi7pXb"
      }
    }
  ]
}